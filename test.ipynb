{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae21fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from causalboundingengine.algorithms.tianpearl import TianPearl\n",
    "from causalboundingengine.algorithms.manski import Manski\n",
    "from causalboundingengine.scenarios import BinaryConf\n",
    "from causalboundingengine.scenarios import ContIV\n",
    "from causalboundingengine.scenarios import BinaryIV\n",
    "import numpy as np\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a717bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Large-Scale Comparison Study ===\n",
      "Analyzing dataset 1: Small, Weak confounding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing dataset 2: Small, Strong confounding\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "p < 0, p > 1 or p contains NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset_configs):\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalyzing dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 66\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfounding_strength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconfounding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     dataset_results \u001b[38;5;241m=\u001b[39m analyze_dataset(X, Y, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# Add dataset metadata\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 11\u001b[0m, in \u001b[0;36mgenerate_dataset\u001b[1;34m(n, confounding_strength, seed)\u001b[0m\n\u001b[0;32m      8\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m     10\u001b[0m U \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mbinomial(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, n)\n\u001b[1;32m---> 11\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfounding_strength\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m Y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mbinomial(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.3\u001b[39m \u001b[38;5;241m*\u001b[39m X \u001b[38;5;241m+\u001b[39m confounding_strength \u001b[38;5;241m*\u001b[39m U, n)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, Y\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:3483\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.binomial\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_common.pyx:396\u001b[0m, in \u001b[0;36mnumpy.random._common.check_array_constraint\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_common.pyx:382\u001b[0m, in \u001b[0;36mnumpy.random._common._check_array_cons_bounded_0_1\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: p < 0, p > 1 or p contains NaNs"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from causalboundingengine.scenarios import BinaryConf\n",
    "import time\n",
    "\n",
    "def generate_dataset(n, confounding_strength=0.5, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    U = np.random.binomial(1, 0.5, n)\n",
    "    X = np.random.binomial(1, 0.3 + confounding_strength * U, n)\n",
    "    Y = np.random.binomial(1, 0.2 + 0.3 * X + confounding_strength * U, n)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def analyze_dataset(X, Y, dataset_id):\n",
    "    scenario = BinaryConf(X, Y)\n",
    "    algorithms = ['manski', 'tianpearl', 'autobound']\n",
    "\n",
    "    results = []\n",
    "    for alg_name in algorithms:\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            alg_func = getattr(scenario.ATE, alg_name)\n",
    "            bounds = alg_func()\n",
    "            end_time = time.time()\n",
    "\n",
    "            results.append({\n",
    "                'dataset_id': dataset_id,\n",
    "                'algorithm': alg_name,\n",
    "                'lower_bound': bounds[0],\n",
    "                'upper_bound': bounds[1],\n",
    "                'width': bounds[1] - bounds[0],\n",
    "                'computation_time': end_time - start_time,\n",
    "                'success': True\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'dataset_id': dataset_id,\n",
    "                'algorithm': alg_name,\n",
    "                'lower_bound': None,\n",
    "                'upper_bound': None,\n",
    "                'width': None,\n",
    "                'computation_time': None,\n",
    "                'success': False\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run comparison study\n",
    "print(\"=== Large-Scale Comparison Study ===\")\n",
    "\n",
    "# Generate multiple datasets\n",
    "datasets = []\n",
    "dataset_configs = [\n",
    "    {'n': 100, 'confounding': 0.2, 'name': 'Small, Weak confounding'},\n",
    "    {'n': 100, 'confounding': 0.8, 'name': 'Small, Strong confounding'},\n",
    "    {'n': 1000, 'confounding': 0.2, 'name': 'Large, Weak confounding'},\n",
    "    {'n': 1000, 'confounding': 0.8, 'name': 'Large, Strong confounding'},\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "for i, config in enumerate(dataset_configs):\n",
    "    print(f\"Analyzing dataset {i+1}: {config['name']}\")\n",
    "\n",
    "    X, Y = generate_dataset(\n",
    "        n=config['n'],\n",
    "        confounding_strength=config['confounding'],\n",
    "        seed=42 + i\n",
    "    )\n",
    "\n",
    "    dataset_results = analyze_dataset(X, Y, i+1)\n",
    "\n",
    "    # Add dataset metadata\n",
    "    for result in dataset_results:\n",
    "        result.update({\n",
    "            'sample_size': config['n'],\n",
    "            'confounding_strength': config['confounding'],\n",
    "            'dataset_name': config['name']\n",
    "        })\n",
    "\n",
    "    all_results.extend(dataset_results)\n",
    "\n",
    "# Compile results\n",
    "df_results = pd.DataFrame(all_results)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\\\n=== Summary Results ===\")\n",
    "summary = df_results[df_results['success']].groupby(['algorithm', 'confounding_strength']).agg({\n",
    "    'width': ['mean', 'std'],\n",
    "    'computation_time': ['mean', 'std']\n",
    "}).round(4)\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Best performing algorithm by scenario\n",
    "print(\"\\\\n=== Best Algorithm by Scenario (narrowest bounds) ===\")\n",
    "best_by_scenario = df_results[df_results['success']].loc[\n",
    "    df_results[df_results['success']].groupby(['dataset_id'])['width'].idxmin()\n",
    "][['dataset_name', 'algorithm', 'width']]\n",
    "\n",
    "print(best_by_scenario.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "706253e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_compute_bounds(scenario, algorithm_name, query='ATE', **kwargs):\n",
    "    \"\"\"Safely compute bounds with fallback.\"\"\"\n",
    "    try:\n",
    "        dispatcher = getattr(scenario, query)\n",
    "        algorithm = getattr(dispatcher, algorithm_name)\n",
    "        return algorithm(**kwargs)\n",
    "    except (AttributeError, ImportError) as e:\n",
    "        print(f\"Algorithm {algorithm_name} not available: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "686dd04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-0.48333333), np.float64(-0.48333333))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = np.array([0, 1, 1, 0, 1])  # Example instrumental variable data\n",
    "X = np.array([0, 1, 1, 0, 1])  # Example treatment data\n",
    "Y = np.array([1, 0, 0.1, 0.5, 0.7])  # Example outcome data\n",
    "\n",
    "\n",
    "\n",
    "scenario = ContIV(X, Y, Z) # Instantiate the scenario with the data\n",
    "scenario.ATE.zhangbareinboim() # Call the Zaffalon bounds method"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
